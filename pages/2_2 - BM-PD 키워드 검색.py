import io
import pandas as pd
import streamlit as st
from datetime import date
import openpyxl
import re

from utils import KIND_OPTIONS, SITE_OPTIONS, PROCESS_OPTIONS, UNIT_OPTIONS
from utils import load_sheet_data

# â€” Streamlit UI
st.set_page_config(page_title="BM/PD ë‚´ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰", layout="wide")
st.title("BM/PD ë‚´ì—­ í‚¤ì›Œë“œ ê²€ìƒ‰")

# 1) ì‚¬ì´íŠ¸ ì„ íƒ
selected_site = st.selectbox("ğŸ” ë¶„ì„í•  ì‚¬ì´íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”", SITE_OPTIONS)

# 2) ì „ì²´ ë°ì´í„° ë¡œë“œ
df, error = load_sheet_data()
if error:
    st.error(error)
    st.stop()

# 3) â€˜siteâ€™ ì»¬ëŸ¼ ìë™ ê°ì§€
cols = df.columns.tolist()
if "site" in cols:
    site_col = "site"
else:
    candidates = [c for c in cols if "ì‚¬ì´íŠ¸" in c or "Site" in c or "SITE" in c]
    if candidates:
        site_col = candidates[0]
        st.warning(f"â€˜{site_col}â€™ ì»¬ëŸ¼ì„ ì‚¬ì´íŠ¸ ì‹ë³„ìš©ìœ¼ë¡œ ìë™ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.error(f"ì‚¬ì´íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nâ†’ í˜„ì¬ ì»¬ëŸ¼ ëª©ë¡: {cols}")
        st.stop()

# 4) ì„ íƒëœ ì‚¬ì´íŠ¸ë¡œ í•„í„°ë§
df_site = df[df[site_col] == selected_site].reset_index(drop=True)

# â€” ë‚ ì§œ í•„í„° ì¶”ê°€ ì‹œì‘ â€”
df_site['ë°œìƒì‹œê°„'] = pd.to_datetime(df_site['ë°œìƒì‹œê°„'])
min_date = df_site['ë°œìƒì‹œê°„'].min().date()
max_date = df_site['ë°œìƒì‹œê°„'].max().date()
start_date, end_date = st.date_input(
    "ğŸ“… ê¸°ê°„ ì„ íƒ (ë°œìƒì‹œê°„ ê¸°ì¤€)",
    value=(min_date, max_date),
    min_value=min_date,
    max_value=max_date
)
df_site = df_site[
    (df_site['ë°œìƒì‹œê°„'].dt.date >= start_date) &
    (df_site['ë°œìƒì‹œê°„'].dt.date <= end_date)
].reset_index(drop=True)
# â€” ë‚ ì§œ í•„í„° ì¶”ê°€ ë â€”

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BM25 ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import io
import pandas as pd
import streamlit as st
from janome.tokenizer import Tokenizer
from rank_bm25 import BM25Okapi

# â€” (ì´ì „ ë¡œì§: ì‚¬ì´íŠ¸ ì„ íƒ, df_site ì¤€ë¹„) â€”
import io
import pandas as pd
import streamlit as st
from janome.tokenizer import Tokenizer
from rank_bm25 import BM25Okapi

# â€” (ì´ì „ ë¡œì§: ì‚¬ì´íŠ¸ ì„ íƒ, df_site ì¤€ë¹„) â€”

SEARCH_COLS = ["Machine", "Unit", "Assy'", "í˜„ìƒ", "ì›ì¸", "ì¡°ì¹˜"]

@st.cache_data
def make_corpus(df: pd.DataFrame, cols: list[str]) -> list[str]:
    return df[cols].fillna("").astype(str).agg(" ".join, axis=1).tolist()

@st.cache_resource
def build_bm25(corpus: list[str]) -> BM25Okapi:
    tokenizer = Tokenizer()
    tokenized = [
        [t.surface for t in tokenizer.tokenize(doc)]
        for doc in corpus
    ]
    return BM25Okapi(tokenized)

@st.cache_resource
def build_tokenized_corpus(corpus: list[str]) -> list[list[str]]:
    tokenizer = Tokenizer()
    return [
        [t.surface for t in tokenizer.tokenize(doc)]
        for doc in corpus
    ]

# 1) ë¬¸ì„œ ì¤€ë¹„
docs = make_corpus(df_site, SEARCH_COLS)

# 2) BM25 ëª¨ë¸ ë° í† í°í™”ëœ ì½”í¼ìŠ¤ ìƒì„±
bm25 = build_bm25(docs)
tokenized_corpus = build_tokenized_corpus(docs)

# 3) ê²€ìƒ‰ UI
st.markdown("---")
st.subheader("í†µí•© í…ìŠ¤íŠ¸ ê²€ìƒ‰")
# top_k = st.slider("ê²°ê³¼ ê°œìˆ˜", 1, 100, 5)

# query = st.text_input("ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥")
# use_simple = st.checkbox("âœ… í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ ê²€ìƒ‰ (ë‹¨ìˆœ í•„í„°)", value=False)

##
top_k = st.slider("ê²°ê³¼ ê°œìˆ˜", 1, 100, 5)

# 1) ì‚¬ìš©ì ì…ë ¥ì„ ì‰¼í‘œ ë˜ëŠ” ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
raw_query = st.text_input("ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥ (ì‰¼í‘œ ë˜ëŠ” ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì—¬ëŸ¬ ê°œ ì…ë ¥)")
use_simple = st.checkbox("âœ… í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ ê²€ìƒ‰ (ë‹¨ìˆœ í•„í„°)", value=False)

if raw_query:
    # ',' ë˜ëŠ” ê³µë°±ìœ¼ë¡œ split
    query_list = [q.strip() for q in re.split(r"[,\s]+", raw_query) if q.strip()]
    
    # BM25ìš© í† í°: ëª¨ë“  í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ í•©ì³ ë‹¤ì‹œ í† í°í™”
    tokenizer = Tokenizer()
    combined = " ".join(query_list)
    q_tokens = [t.surface for t in tokenizer.tokenize(combined)]

    if use_simple:
        # ë‹¨ìˆœ í•„í„°: ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ
        matched_idx = [
            i for i, tokens in enumerate(tokenized_corpus)
            if all(q in tokens for q in query_list)
        ][:top_k]
        scores = [None] * len(matched_idx)
    else:
        # BM25 ì ìˆ˜ ê³„ì‚°
        scores_all = bm25.get_scores(q_tokens)
        ranked = sorted(
            enumerate(scores_all),
            key=lambda x: x[1],
            reverse=True
        )
        # ì ìˆ˜ 0 ì´í•˜ ì œê±°
        ranked = [(i, s) for i, s in ranked if s > 0]
        if ranked:
            matched_idx, scores = zip(*ranked[:top_k])
        else:
            matched_idx, scores = [], []

    if not matched_idx:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result_df = df_site.iloc[list(matched_idx)].copy().reset_index(drop=True)
        result_df["Score"] = scores
        # result_df = result_df.sort_values(by="ë°œìƒì‹œê°„").reset_index(drop=True)
        result_df = result_df.sort_values(by="Score", ascending=False).reset_index(drop=True)
        st.table(result_df)

        csv_data = result_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", data=csv_data,
                           file_name="search_results.csv", mime="text/csv")